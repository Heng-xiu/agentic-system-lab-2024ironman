{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YL3sSzhNYrrL"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¥± LangGraph_-Adaptive-RAG ç¯‡\n",
        "\n",
        "## é©æ‡‰æ€§æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆAdaptive RAGï¼‰ç­–ç•¥\n",
        "\n",
        "é©æ‡‰æ€§æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆAdaptive RAGï¼‰æ˜¯ä¸€ç¨®å…ˆé€²çš„æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRAGï¼‰ç­–ç•¥ï¼Œå®ƒçµåˆäº†å…©å€‹é—œéµå…ƒç´ ï¼š\n",
        "\n",
        "1. æŸ¥è©¢åˆ†æ\n",
        "2. ä¸»å‹•å¼ï¼è‡ªæˆ‘ä¿®æ­£å¼ RAG\n",
        "\n",
        "æ ¹æ“šç›¸é—œç ”ç©¶è«–æ–‡ï¼ŒæŸ¥è©¢åˆ†æå¯ç”¨æ–¼åœ¨ä»¥ä¸‹ä¸‰ç¨®æ¨¡å¼é–“é€²è¡Œè·¯ç”±é¸æ“‡ï¼š\n",
        "\n",
        "* ç„¡æª¢ç´¢æ¨¡å¼\n",
        "* å–®æ¬¡ RAG æ¨¡å¼\n",
        "* è¿­ä»£å¼ RAG æ¨¡å¼\n",
        "\n",
        "æˆ‘å€‘å°‡åˆ©ç”¨ LangGraph å·¥å…·ä¾†æ“´å±•é€™å€‹æ¦‚å¿µã€‚åœ¨æˆ‘å€‘çš„å¯¦ä½œä¸­ï¼Œæˆ‘å€‘å°‡åœ¨ä»¥ä¸‹å¹¾ç¨®æ–¹å¼é–“é€²è¡Œè·¯ç”±ï¼š\n",
        "\n",
        "* ç¶²è·¯æœå°‹ï¼šç”¨æ–¼è™•ç†èˆ‡è¿‘æœŸäº‹ä»¶ç›¸é—œçš„å•é¡Œ\n",
        "* è‡ªæˆ‘ä¿®æ­£å¼ RAGï¼šç”¨æ–¼è™•ç†èˆ‡æˆ‘å€‘ç´¢å¼•ç›¸é—œçš„å•é¡Œ\n",
        "\n",
        "é€™ç¨®æ–¹æ³•èƒ½å¤ æ ¹æ“šä¸åŒé¡å‹çš„æŸ¥è©¢ï¼Œéˆæ´»åœ°é¸æ“‡æœ€åˆé©çš„è™•ç†æ–¹å¼ï¼Œå¾è€Œæé«˜å›ç­”çš„æº–ç¢ºæ€§å’Œç›¸é—œæ€§ã€‚\n",
        "\n",
        "â¤ï¸ Created by [hengshiousheu](https://huggingface.co/Heng666)."
      ],
      "metadata": {
        "id": "sp24U2f_Xlqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![adaptive-rag.png](https://i.imgur.com/r7mXnu3.png)\n",
        "[åœ–æºå–è‡ª Langgraph å®˜ç¶²](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/)"
      ],
      "metadata": {
        "id": "zQNpYjvz9UAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ç’°å¢ƒè¨­ç½®\n",
        "## å®‰è£å¿…è¦å¥—ä»¶\n",
        "é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦å®‰è£ä¸€äº›é—œéµçš„ Python å¥—ä»¶ï¼š"
      ],
      "metadata": {
        "id": "zfmTSQLoXoIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain_community\n",
        "%pip install --upgrade --quiet tiktoken\n",
        "%pip install --upgrade --quiet langchain-openai\n",
        "%pip install --upgrade --quiet langchainhub\n",
        "%pip install --upgrade --quiet chromadb\n",
        "%pip install --upgrade --quiet langchain\n",
        "%pip install --upgrade --quiet langgraph\n",
        "%pip install --upgrade --quiet tavily-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkYoy4I35CA0",
        "outputId": "49c8b396-c397-4b38-8ec1-66c063782717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.1/149.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m427.7/427.7 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.1/87.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## è¨­ç½® API é‡‘é‘°\n",
        "æˆ‘å€‘éœ€è¦è¨­ç½® ç’°å¢ƒè®Šæ•¸ OPENAI_API_KEY ï¼Œå¯ä»¥ç›´æ¥å®Œæˆï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š"
      ],
      "metadata": {
        "id": "zoEZJKLqYD0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['TAVILY_API_KEY'] = userdata.get('TAVILY_API_KEY')"
      ],
      "metadata": {
        "id": "fktPg5voEllg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ğŸ”’ è«‹æ³¨æ„ä¿è­·å¥½ä½ çš„ API é‡‘é‘°ï¼Œé¿å…æ´©éœ²æˆ–æ¿«ç”¨ã€‚"
      ],
      "metadata": {
        "id": "yLFS9xksYGm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (å¯ç”¨å¯ä¸ç”¨)LangSmith\n",
        "\n",
        "ä½ ç”¨LangChainæ§‹å»ºçš„è¨±å¤šæ‡‰ç”¨ç¨‹å¼å°‡åŒ…å«å¤šå€‹æ­¥é©Ÿï¼Œä¸¦å¤šæ¬¡èª¿ç”¨LLMèª¿ç”¨ã€‚éš¨è‘—é€™äº›æ‡‰ç”¨ç¨‹å¼è®Šå¾—è¶Šä¾†è¶Šè¤‡é›œï¼Œèƒ½å¤ æª¢æŸ¥æ‚¨çš„éˆæˆ–ä»£ç†å…§éƒ¨åˆ°åº•ç™¼ç”Ÿäº†ä»€éº¼è®Šå¾—è‡³é—œé‡è¦ã€‚æœ€å¥½çš„æ–¹æ³•æ˜¯èˆ‡[LangSmith](https://smith.langchain.com)åˆä½œã€‚\n",
        "\n",
        "è«‹æ³¨æ„ï¼ŒLangSmith ä¸æ˜¯å¿…éœ€çš„ï¼Œä½†å®ƒå¾ˆæœ‰èª¬æ˜ã€‚å¦‚æœæ‚¨ç¢ºå¯¦æƒ³ä½¿ç”¨ LangSmithï¼Œè«‹åœ¨ä¸Šé¢çš„éˆæ¥ä¸­è¨»å†Šåï¼Œè«‹ç¢ºä¿è¨­ç½®ç’°å¢ƒè®Šæ•¸ä»¥é–‹å§‹è¨˜éŒ„è·Ÿè¹¤ï¼š"
      ],
      "metadata": {
        "id": "YL3sSzhNYrrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "current_time = datetime.now(pytz.timezone('Asia/Taipei')).strftime(\"%Y-%m-%d %Z\")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\" ##æƒ³è¦ä½¿ç”¨è¨˜å¾—æ”¹ true, ä¸è¦æ™‚æ”¹ false.\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangGraph Adaptive-RAG-{current_time}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')"
      ],
      "metadata": {
        "id": "vQ-i-oPPYtgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å»ºç«‹å‘é‡è³‡æ–™åº«\n",
        "\n",
        "ç•¢ç«Ÿæ˜¯ RAG ï¼Œä½ æ‡‚çš„ã€‚åœ¨æ­¤ç”¨è¨˜æ†¶é«”è³‡æ–™åº« `Chroma` å”åŠ©æˆ‘å€‘å®Œæˆç¯„ä¾‹ã€‚å¯¦éš›æ¡ˆä¾‹ä¸­ï¼Œæœƒä¾ç…§å°ˆæ¡ˆä¸åŒé¸ç”¨ä¸åŒçš„å‘é‡è³‡æ–™åº«ä»¥åŠä¿å­˜æ–¹å¼ã€‚"
      ],
      "metadata": {
        "id": "7b00LfpzY4HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Build Index\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import LanceDB\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "\n",
        "###### router import\n",
        "\n",
        "from typing import Literal\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langchain.schema import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "embedding_function = OpenAIEmbeddings()\n",
        "\n",
        "urls = [\n",
        "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "        \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "    ]\n",
        "\n",
        "# Load\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        " # Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=512, chunk_overlap=0\n",
        "    )\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Add to vectorstore\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    embedding=embedding_function\n",
        "    )\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "print(\"vectoselected\",vectorstore)\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p0y2OGlu8OV",
        "outputId": "1b84d685-3d90-418a-ff13-189696160cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vectoselected <langchain_community.vectorstores.chroma.Chroma object at 0x787070d1a830>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "é€™å€‹æ­¥é©Ÿå‰µå»ºäº†ä¸€å€‹åŒ…å«ç›¸é—œæ–‡æª”çš„å‘é‡è³‡æ–™åº«ï¼Œç‚ºå¾ŒçºŒçš„æª¢ç´¢æ“ä½œåšå¥½æº–å‚™ã€‚"
      ],
      "metadata": {
        "id": "a4QBorwZ_xiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## æ§‹å»ºç³»çµ±çµ„ä»¶"
      ],
      "metadata": {
        "id": "md3__EA6_ywk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### è·¯ç”±å™¨ï¼ˆRouterï¼‰\n",
        "\n",
        "è·¯ç”±å™¨ç”¨æ–¼æ±ºå®šæ˜¯ä½¿ç”¨å‘é‡å­˜å„²é‚„æ˜¯ç¶²çµ¡æœç´¢ä¾†å›ç­”å•é¡Œï¼š"
      ],
      "metadata": {
        "id": "-EEWVkkwZfWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Build Index\n",
        "### Router\n",
        "\n",
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Data model\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n",
        "    )\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"ä½ æ˜¯ä¸€å€‹å°ˆå®¶ï¼Œè² è²¬å°‡ç”¨æˆ¶å•é¡Œè·¯ç”±åˆ°å‘é‡å­˜å„²æˆ–ç¶²çµ¡æœç´¢ã€‚\n",
        "å‘é‡å­˜å„²åŒ…å«èˆ‡ä»£ç†ã€æç¤ºå·¥ç¨‹å’Œå°æŠ—æ€§æ”»æ“Šç›¸é—œçš„æ–‡æª”ã€‚\n",
        "å°æ–¼é€™äº›ä¸»é¡Œçš„å•é¡Œä½¿ç”¨å‘é‡å­˜å„²ã€‚å¦å‰‡ï¼Œä½¿ç”¨ç¶²çµ¡æœç´¢ã€‚\"\"\"\n",
        "route_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_router = route_prompt | structured_llm_router\n",
        "\n",
        "\n",
        "question1 =\"æå¤šæ…§ä»€éº¼æ™‚å€™åŠ å…¥çµ±ä¸€ç…ï¼Ÿ\"\n",
        "print(question_router.invoke({\"question\": question1}))\n",
        "print(question_router.invoke({\"question\": \"è«‹å• Agent æ˜¯ä»€éº¼ï¼Ÿ\"}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S72zdGy2vPlb",
        "outputId": "34af3b4a-706e-486e-d5e0-dd80622cb51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasource='web_search'\n",
            "datasource='vectorstore'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## æª¢ç´¢è©•åˆ†å™¨\n",
        "é€™å€‹çµ„ä»¶ç”¨æ–¼è©•ä¼°æª¢ç´¢æ–‡æª”èˆ‡å•é¡Œçš„ç›¸é—œæ€§ï¼š"
      ],
      "metadata": {
        "id": "rbCfWEGZ9wSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Retrieval Grader\n",
        "\n",
        "# Data model\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"ä½ æ˜¯ä¸€å€‹è©•åˆ†å“¡ï¼Œè©•ä¼°æª¢ç´¢æ–‡æª”èˆ‡ç”¨æˆ¶å•é¡Œçš„ç›¸é—œæ€§ã€‚ \\n\n",
        "    å¦‚æœæ–‡æª”åŒ…å«èˆ‡ç”¨æˆ¶å•é¡Œç›¸é—œçš„é—œéµè©æˆ–èªç¾©å«ç¾©ï¼Œå°‡å…¶è©•ç‚ºç›¸é—œã€‚ \\n\n",
        "    é€™ä¸éœ€è¦æ˜¯åš´æ ¼çš„æ¸¬è©¦ã€‚ç›®çš„æ˜¯éæ¿¾æ‰éŒ¯èª¤çš„æª¢ç´¢çµæœã€‚ \\n\n",
        "    çµ¦å‡ºäºŒå…ƒè©•åˆ† 'yes' æˆ– 'no' ä¾†è¡¨ç¤ºæ–‡æª”æ˜¯å¦èˆ‡å•é¡Œç›¸é—œã€‚\"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"æª¢ç´¢æ–‡æª”ï¼š\\n\\n {document} \\n\\n ç”¨æˆ¶å•é¡Œï¼š{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n",
        "question = \"agent memory\"\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "doc_txt = docs[1].page_content\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
      ],
      "metadata": {
        "id": "67NphMjHZN5M",
        "outputId": "b77b783b-010d-4c1d-e151-b1e579fafad4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "binary_score='no'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## æ¨¡æ“¬æª¢ç´¢æˆåŠŸçš„æ–‡æª”"
      ],
      "metadata": {
        "id": "MZR8u4u9GFUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Document:\n",
        "    def __init__(self, page_content):\n",
        "        self.page_content = page_content\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Document(page_content='{self.page_content}')\"\n",
        "\n",
        "cleaned_docs_format = [Document(doc.page_content) for doc in docs]\n",
        "\n",
        "for doc in cleaned_docs_format:\n",
        "    print(doc)"
      ],
      "metadata": {
        "id": "Nq11QGniGIkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ç”Ÿæˆç¯€é»\n",
        "ç”¨é€”ï¼šç”Ÿæˆç¯€é»è² è²¬åŸºæ–¼æª¢ç´¢çš„æ–‡æª”ç”Ÿæˆç­”æ¡ˆ"
      ],
      "metadata": {
        "id": "Pm5_TFuR95D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          (\"human\", \"\"\"ä½ æ˜¯ä¸€å€‹å•ç­”ä»»å‹™çš„åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹æª¢ç´¢çš„ä¸Šä¸‹æ–‡ä¾†å›ç­”å•é¡Œã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±ç›´èªªä¸çŸ¥é“ã€‚æœ€å¤šä½¿ç”¨ä¸‰å€‹å¥å­ï¼Œä¿æŒç­”æ¡ˆç°¡æ½”ã€‚\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Answer:\n",
        "\"\"\"),\n",
        "      ]\n",
        ")\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# # Run\n",
        "generation = rag_chain.invoke({\"context\": cleaned_docs_format, \"question\": question})\n",
        "print(generation)"
      ],
      "metadata": {
        "id": "TGKO4lBUZN74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28c9753f-3c2c-4995-fc63-86ce74affa65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The agent memory module is a long-term memory database that records agents' experiences in natural language. It includes observations and events provided by the agent, which can trigger new natural language statements through inter-agent communication. The retrieval model surfaces context based on relevance, recency, and importance to inform the agent's behavior.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å¹»è¦ºè©•åˆ†å™¨\n",
        "é€™å€‹çµ„ä»¶ç”¨æ–¼è©•ä¼°ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦åŸºæ–¼äº‹å¯¦ï¼š"
      ],
      "metadata": {
        "id": "m5BODaie98lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Hallucination Grader\n",
        "\n",
        "# Data model\n",
        "class GradeHallucinations(BaseModel):\n",
        "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"ä½ æ˜¯ä¸€å€‹è©•åˆ†å“¡ï¼Œè©•ä¼° LLM ç”Ÿæˆçš„å…§å®¹æ˜¯å¦åŸºæ–¼/æ”¯æŒæ–¼ä¸€çµ„æª¢ç´¢çš„äº‹å¯¦ã€‚ \\n\n",
        "     çµ¦å‡ºäºŒå…ƒè©•åˆ† 'yes' æˆ– 'no'ã€‚'Yes' è¡¨ç¤ºç­”æ¡ˆåŸºæ–¼/æ”¯æŒæ–¼é€™çµ„äº‹å¯¦ã€‚\"\"\"\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"äº‹å¯¦é›†ï¼š\\n\\n {documents} \\n\\n LLM ç”Ÿæˆï¼š{generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "hallucination_grader.invoke({\"documents\": cleaned_docs_format, \"generation\": generation})"
      ],
      "metadata": {
        "id": "fZ39RkqtZN-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d600b4e0-3af7-445e-eead-ffa4495d6e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradeHallucinations(binary_score='yes')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ç­”æ¡ˆè©•åˆ†å™¨\n",
        "ç”¨æ–¼è©•ä¼°ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦è§£æ±ºäº†å•é¡Œï¼š"
      ],
      "metadata": {
        "id": "hgJ8wN0d-C1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Answer Grader\n",
        "\n",
        "# Data model\n",
        "class GradeAnswer(BaseModel):\n",
        "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"ä½ æ˜¯ä¸€å€‹è©•åˆ†å“¡ï¼Œè©•ä¼°ç­”æ¡ˆæ˜¯å¦è§£æ±º/å›ç­”äº†å•é¡Œã€‚\\n\n",
        "çµ¦å‡ºäºŒå…ƒè©•åˆ† 'yes' æˆ– 'no'ã€‚'Yes' è¡¨ç¤ºç­”æ¡ˆè§£æ±ºäº†å•é¡Œã€‚\"\"\"\n",
        "answer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"ç”¨æˆ¶å•é¡Œï¼š\\n\\n {question} \\n\\n LLM ç”Ÿæˆï¼š{generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer_grader = answer_prompt | structured_llm_grader\n",
        "answer_grader.invoke({\"question\": question,\"generation\": generation})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkQTpm2fD53I",
        "outputId": "a5c23632-a708-4950-bf95-eda85140bef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradeAnswer(binary_score='yes')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## æŸ¥è©¢å„ªåŒ–å™¨\n",
        "é€™å€‹çµ„ä»¶ç”¨æ–¼æ”¹å¯«ç”¨æˆ¶å•é¡Œï¼Œä½¿å…¶æ›´é©åˆæª¢ç´¢ï¼š"
      ],
      "metadata": {
        "id": "RUnzdz8v-KZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Question Re-writer\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"ä½ æ˜¯ä¸€å€‹å•é¡Œé‡å¯«å™¨ï¼Œå°‡è¼¸å…¥å•é¡Œè½‰æ›ç‚ºæ›´é©åˆå‘é‡å­˜å„²æª¢ç´¢çš„ç‰ˆæœ¬ã€‚\\n\n",
        "åˆ†æè¼¸å…¥ä¸¦å˜—è©¦ç†è§£å…¶åº•å±¤èªç¾©æ„åœ–/å«ç¾©ã€‚\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"é€™æ˜¯åˆå§‹å•é¡Œï¼š\\n\\n {question} \\n åˆ¶å®šä¸€å€‹æ”¹é€²çš„å•é¡Œã€‚\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "question_rewriter.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "b1OUFY3sZOBZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "18774400-6db3-44ee-85fe-667aafd8d99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'æ”¹é€²å¾Œçš„å•é¡Œï¼š\\n\\nå¦‚ä½•æœ‰æ•ˆåœ°ç®¡ç†ä»£ç†äººçš„è¨˜æ†¶ï¼Ÿ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Search\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "web_search_tool = TavilySearchResults(k=3)"
      ],
      "metadata": {
        "id": "fVyehT7sEKMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å®šç¾©åœ–çš„ç‹€æ…‹"
      ],
      "metadata": {
        "id": "11713gIH-Rzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "#langgraph code\n",
        "from typing_extensions import TypedDict\n",
        "from typing import List\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "    question : str\n",
        "    generation : str\n",
        "    documents : List[str]\n"
      ],
      "metadata": {
        "id": "1SJqOmndZOJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å®šç¾©ç¯€é»å‡½æ•¸\n",
        "\n",
        "æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘å®šç¾©äº†å¹¾å€‹é—œéµçš„ç¯€é»å‡½æ•¸ï¼ŒåŒ…æ‹¬æª¢ç´¢ã€ç”Ÿæˆã€æ–‡æª”è©•åˆ†ã€æŸ¥è©¢è½‰æ›å’Œç¶²çµ¡æœç´¢ã€‚é€™äº›å‡½æ•¸å°‡åœ¨æˆ‘å€‘çš„åœ–ä¸­ä½œç‚ºç¯€é»ä½¿ç”¨ã€‚"
      ],
      "metadata": {
        "id": "um1NgmFW-bWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## graph flow\n",
        "from langchain.schema import Document\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---[æª¢ç´¢æ–‡ä»¶]---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.invoke(question)\n",
        "\n",
        "    # docs = retriever.get_relevant_documents(question)\n",
        "    # documents = [doc.page_content for doc in docs]\n",
        "\n",
        "    # docsNEW = [doc.page_content for doc in docs]  # Adjust if the actual attribute name differs\n",
        "    print(\"TAKING DICSNEW BRO...>>>>>>>>\")\n",
        "\n",
        "    return {\"documents\": cleaned_docs_format, \"question\": question}\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---[ç”Ÿæˆç­”æ¡ˆ]---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with only filtered relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---[æª¢æŸ¥æ–‡æª”èˆ‡æŸ¥è©¢ç›¸é—œæ€§]---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---[è©•åˆ†]: æ–‡æª”ç›¸é—œ---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---[è©•åˆ†]: æ–‡æª”ä¸ç›¸é—œ---\")\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question}\n",
        "\n",
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---[é‡å¯«æŸ¥è©¢]---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Re-write question\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    return {\"documents\": documents, \"question\": better_question}\n",
        "\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based on the re-phrased question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with appended web results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---[ç¶²è·¯æª¢ç´¢]---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Web search\n",
        "    docs = web_search_tool.invoke({\"query\": question})\n",
        "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "    web_results = Document(page_content=web_results)\n",
        "\n",
        "    return {\"documents\": web_results, \"question\": question}\n"
      ],
      "metadata": {
        "id": "qp4tONlW-feu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å®šç¾©æµç¨‹æ§åˆ¶å‡½æ•¸\n",
        "æˆ‘å€‘é‚„å®šç¾©äº†ä¸€äº›æµç¨‹æ§åˆ¶å‡½æ•¸ï¼Œç”¨æ–¼æ±ºå®šä¸‹ä¸€æ­¥æ‡‰è©²åŸ·è¡Œå“ªå€‹ç¯€é»ã€‚é€™äº›å‡½æ•¸åŒ…æ‹¬å•é¡Œè·¯ç”±ã€ç”Ÿæˆæ±ºç­–å’Œç”Ÿæˆçµæœè©•ä¼°ã€‚"
      ],
      "metadata": {
        "id": "QaOpBz2r-fRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Edges ###\n",
        "\n",
        "def route_question(state):\n",
        "    \"\"\"\n",
        "    Route question to web search or RAG.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---[å•é¡Œåˆ†é¡]---\")\n",
        "    question = state[\"question\"]\n",
        "    source = question_router.invoke({\"question\": question})\n",
        "    if source.datasource == 'web_search':\n",
        "        print(\"---[å•é¡Œåˆ†é¡->ç¶²è·¯æª¢ç´¢]---\")\n",
        "        return \"web_search\"\n",
        "    elif source.datasource == 'vectorstore':\n",
        "        print(\"---[å•é¡Œåˆ†é¡->RAG]---\")\n",
        "        return \"vectorstore\"\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-generate a question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    question = state[\"question\"]\n",
        "    filtered_documents = state[\"documents\"]\n",
        "\n",
        "    if not filtered_documents:\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\"\n",
        "\n",
        "def grade_generation_v_documents_and_question(state):\n",
        "    \"\"\"\n",
        "    Determines whether the generation is grounded in the document and answers question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK HALLUCINATIONS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "\n",
        "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
        "    grade = score.binary_score\n",
        "\n",
        "    # Check hallucination\n",
        "    if grade == \"yes\":\n",
        "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "        # Check question-answering\n",
        "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "            return \"useful\"\n",
        "        else:\n",
        "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "            return \"not useful\"\n",
        "    else:\n",
        "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "        return \"not supported\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4SZmdvtOZOMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å»ºç½®èˆ‡ç·¨è­¯æ•´å¼µåœ–\n",
        "\n",
        "æœ€å¾Œï¼Œæˆ‘å€‘ä½¿ç”¨ Langgraph çš„ StateGraph ä¾†æ§‹å»ºæˆ‘å€‘çš„é©æ‡‰æ€§ RAG ç³»çµ±ï¼š\n"
      ],
      "metadata": {
        "id": "Gbia8AtM-lQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## grapj build\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"web_search\", web_search) # web search\n",
        "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
        "workflow.add_node(\"generate\", generate) # generatae\n",
        "workflow.add_node(\"transform_query\", transform_query) # transform_query\n",
        "\n",
        "# Build graph\n",
        "workflow.set_conditional_entry_point(\n",
        "    route_question,\n",
        "    {\n",
        "        \"web_search\": \"web_search\",\n",
        "        \"vectorstore\": \"retrieve\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"web_search\", \"generate\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    grade_generation_v_documents_and_question,\n",
        "    {\n",
        "        \"not supported\": \"generate\",\n",
        "        \"useful\": END,\n",
        "        \"not useful\": \"transform_query\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "PGWAX0mN-oN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## èˆ‡åœ–é€²è¡Œäº’å‹•\n",
        "\n",
        "ç¾åœ¨æˆ‘å€‘å¯ä»¥ä½¿ç”¨ç·¨è­¯å¾Œçš„åœ–ä¾†è™•ç†å„ç¨®æŸ¥è©¢ã€‚ä»¥ä¸‹æ˜¯å¹¾å€‹ç¤ºä¾‹ï¼š\n"
      ],
      "metadata": {
        "id": "JaqYUlFV-pmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¡ˆä¾‹ä¸€ã€NFL é¸ç§€ç›¸é—œå•é¡Œ"
      ],
      "metadata": {
        "id": "voVQN3W9-uik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\"question\": \"What player at the Bears expected to draft first in the 2024 NFL draft?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node//\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRMvOuF--rxm",
        "outputId": "10f24f8f-7af0-4248-c849-c0994109eb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---[å•é¡Œåˆ†é¡]---\n",
            "---[å•é¡Œåˆ†é¡->ç¶²è·¯æª¢ç´¢]---\n",
            "---[ç¶²è·¯æª¢ç´¢]---\n",
            "\"Node 'web_search':\"\n",
            "'\\n---\\n'\n",
            "---[ç”Ÿæˆç­”æ¡ˆ]---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "('Caleb Williams from USC was expected to be drafted first by the Bears in the '\n",
            " '2024 NFL draft.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaned_docs_format"
      ],
      "metadata": {
        "id": "nkHSzbwh_4MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¡ˆä¾‹äºŒã€ç”Ÿæˆå¼ä»£ç†ç›¸é—œå•é¡Œ"
      ],
      "metadata": {
        "id": "nmYJ6was-wps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\"question\": \"ä»€éº¼æ˜¯ Agent ?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node//\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cqUImpZ6_-u",
        "outputId": "3771f7b4-e5d4-466f-c76d-f54818d6bdde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---[å•é¡Œåˆ†é¡]---\n",
            "---[å•é¡Œåˆ†é¡->RAG]---\n",
            "---[æª¢ç´¢æ–‡ä»¶]---\n",
            "TAKING DICSNEW BRO...>>>>>>>>\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---[æª¢æŸ¥æ–‡æª”èˆ‡æŸ¥è©¢ç›¸é—œæ€§]---\n",
            "---[è©•åˆ†]: æ–‡æª”ç›¸é—œ---\n",
            "---[è©•åˆ†]: æ–‡æª”ç›¸é—œ---\n",
            "---[è©•åˆ†]: æ–‡æª”ç›¸é—œ---\n",
            "---[è©•åˆ†]: æ–‡æª”ç›¸é—œ---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---[ç”Ÿæˆç­”æ¡ˆ]---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "('åœ¨é€™å€‹ä¸Šä¸‹æ–‡ä¸­ï¼ŒAgent æ˜¯æŒ‡ä¸€ç¨®ç”± GPT-3.5 '\n",
            " 'é©…å‹•çš„ä»£ç†äººï¼Œç”¨æ–¼åŸ·è¡Œç°¡å–®ä»»å‹™å’Œäº’å‹•æ‡‰ç”¨ç¨‹åºã€‚é€™äº›ä»£ç†äººçµåˆäº†èªè¨€æ¨¡å‹ã€è¨˜æ†¶ã€è¦åŠƒå’Œåæ€æ©Ÿåˆ¶ï¼Œä»¥ä¾¿æ ¹æ“šéå»çš„ç¶“é©—è¡Œç‚ºï¼Œä¸¦èˆ‡å…¶ä»–ä»£ç†äººäº’å‹•ã€‚ä»£ç†äººé‚„å¯ä»¥èª¿ç”¨å¤–éƒ¨ '\n",
            " 'API ä»¥ç²å–ç¼ºå¤±çš„ä¿¡æ¯ã€‚')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¡ˆä¾‹ä¸‰ã€ç¾åœ‹ç¸½çµ±ç›¸é—œå•é¡Œ"
      ],
      "metadata": {
        "id": "1aAuZl0e-x5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\"question\": \"èª°æ˜¯ç¾åœ‹å»ºåœ‹ç¸½çµ±ï¼Ÿ\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node//\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Eh2KhT-ZmpW",
        "outputId": "24d8d439-1132-419c-d609-b79448150a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---[å•é¡Œåˆ†é¡]---\n",
            "---[å•é¡Œåˆ†é¡->ç¶²è·¯æª¢ç´¢]---\n",
            "---[ç¶²è·¯æª¢ç´¢]---\n",
            "\"Node 'web_search':\"\n",
            "'\\n---\\n'\n",
            "---[ç”Ÿæˆç­”æ¡ˆ]---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "'å–¬æ²»Â·è¯ç››é “æ˜¯ç¾åœ‹å»ºåœ‹ç¸½çµ±ã€‚'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# çµè«–\n",
        "é€šéé€™å€‹æ•™ç¨‹ï¼Œæˆ‘å€‘å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Langgraph æ§‹å»ºä¸€å€‹é©æ‡‰æ€§ RAG ç³»çµ±ã€‚é€™å€‹ç³»çµ±èƒ½å¤ æ ¹æ“šå•é¡Œçš„æ€§è³ªé¸æ“‡é©ç•¶çš„ä¿¡æ¯ä¾†æºï¼Œè©•ä¼°æª¢ç´¢çµæœçš„ç›¸é—œæ€§ï¼Œä¸¦åœ¨éœ€è¦æ™‚å„ªåŒ–æŸ¥è©¢æˆ–ç”Ÿæˆç­”æ¡ˆã€‚é€™ç¨®æ–¹æ³•å¤§å¤§æé«˜äº†å•ç­”ç³»çµ±çš„éˆæ´»æ€§å’Œæº–ç¢ºæ€§ã€‚"
      ],
      "metadata": {
        "id": "eWZ9jri3BkmG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p3umALgi9YFC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}